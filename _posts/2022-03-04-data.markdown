---
layout: post
title:  "Data Analytics as An Open Problem in Web3"
date:   2022-03-04 09:22:00
---


Everyone living in this era generates data every day via Apps, websites, and IoT devices. Companies behind collect the user data and extract value for business purposes. Data analytics, referring to the process of shaping raw data into desired structures and the subsequent analysis conducted on top, is a bigger problem than you think. In web2, this is almost a closed problem, i.e., we know there is a lot of data, but the data is only constricted to organizations as the top value assets. Tons of database system developers, data engineers, data scientists, and business analysts are playing a part in the data pipeline every moment. Hundreds of big companies and unicorns built their billion-dollar business by solving a piece.

With the rise of permissionless blockchain, the data problem becomes more visible. Anyone can view the on-chain data at websites like Etherscan. And the data analysis process becomes transparent as well. [Dune Analytics](https://dune.xyz) is a platform for data experts to play with the data pulled from blockchains. The engineers and open-source contributors extracted, transformed, and loaded (ETL) blockchain data into tables such as ethereum.transactions and ntf.trades. The tables live in cloud systems. The analysts can write SQL queries, build visualizations and create dashboards upon the data. I have seen many times that the dashboards in Dune Analytics are cited as data sources for blockchain news. There is no such a transparent and informative platform in web2 as far as I know. Platforms like Dune Analytics make me optimistic about how the open data and the data analysts can help build a more efficient web3.

I see there are three key factors driving this change. First and foremost, the data access and the value residing inside the data. Although web3 is decentralized, web3 data is “centralized,” i.e., the transaction histories, which are the most important data, lives on the openly accessible mainstream blockchains. The data is not only but also carries a value of high density. Second, a single machine would not be impossible to handle billions of transaction records. Consuming massive data requires advanced hardware and software resources. Luckily, this problem has been solved thousands of times in web2, and many software are open-source. With tools like Spark, HDFS, DBT, and Google Cloud, a small team can quickly build an effective, customized, and automatic data pipeline that can process massive data. Last but not least, the people. Given how big the data market already is, many people are equipped with the skills to work with data. They can quickly find a place in the pipeline, no matter it is ETL, write SQLs or question-driven analysis. [MetricsDao](https://mirror.xyz/0x3138165f8d21d4869dbD406CD8bc8055CAC8fb6E/cQtRNHKaXr2OnX0X3QNsDGSusykYkw9D5XwY1PHvvZ4) is a DAO formed by such a group of passionate individuals. The DAO distributes all kinds of tasks to members to build an open analytic community, and the members share the belief that the access to data for analysis should be open and free.

Between the open-minded people and open-access data are the open-to-use, centralized, and extremely costly data storage and processing infrastructure. I am curious to see how the value generated can balance out the cost. This is a good point where Web2 meets Web3, in my opinion.